{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uuATYYtzkB2"
      },
      "source": [
        "<font size='10' color = 'E3A440'>**Pre-confèrence Workshop on Python**</font>\n",
        "=======\n",
        "\n",
        "<font color = 'E3A440'>*Pratical introduction to the analysis of unstructured data*</font>\n",
        "=============\n",
        "\n",
        "This tutorial is a short hands-on workshop to introduce the analysis of unstructured data for innovation studies. \n",
        "\n",
        "Structure of the workshop:\n",
        "1. Presentation of sections 1 and 2 in a plenary mode (20 minutes)\n",
        "2. Individual work on sections 2 and 3 (20 minutes)\n",
        "3. Group work on section 3 (60 minutes)\n",
        "4. Plenary section with the group presentation (20 minutes)\n",
        "\n",
        "This tutorial cannot be considered exhaustive of the domain. \n",
        "\n",
        "### Auteurs: \n",
        "- Mikael Heroux-Vaillancourt <mikael.heroux-vaillancourt@polymtl.ca>\n",
        "- Davide Pulizzotto <davide.pulizzotto@polymtl.ca>\n",
        "- Pietro Cruciata <pietro.cruciata@polymtl.ca>\n",
        "- Alvar Herrera <alvar.herrera@polymtl.ca> \n",
        "\n",
        "### Table des matières\n",
        "\n",
        "- [Section 0. Introduction](#introduction)\n",
        "- [Section 1. Preparation of data](#pre-processing)\n",
        "- [Section 2. Descriptive statistics](#desc-stat)\n",
        "- [Section 3. Analysis](#analysis)\n",
        "- [Section 4. Conclusions](#concluding-remarks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "232GZfpEzkB5"
      },
      "source": [
        "<a id='introduction'></a>\n",
        "# <font size = '6' color='E3A440'>Section 0. Introduction</font>\n",
        "\n",
        "This workshop is based on the research work of **Mikael Heroux-Vaillancourt** presented at the [P4IE Conference - Measuring Metrics that Matter](https://event.fourwaves.com/p4ie/pages), which will take place  on -10-11 May 2022 at the *Hilton Garden Inn*, in Ottawa.\n",
        "\n",
        "Here the [link]() to get more information on the presentation.\n",
        "\n",
        "In order to succeed in this workshop, a few concept have to be underlined:\n",
        "\n",
        "1. This study is an exploratory analysis of several companies which obtained a [BCorp certification](https://www.bcorporation.net/en-us/)\n",
        "2. The data analyzed come from a web scraping step of the snapshots found on [Wayback Machine](https://archive.org/web/) of all those companis of point 1.\n",
        "3. Objectives are ... À COMPLETER QUAND MIKAEL NOUS DONNE UN RÉSUMÉ DE SA PRÉSENTATION\n",
        "\n",
        "\n",
        "### Glossary of the workshop\n",
        "\n",
        "Here, we list some terms that are used in this workshop:\n",
        "\n",
        "1. <font color='E3A440'><b>Dataframe</b></font>: a data structure that organizes data into a 2-dimensional table of rows and columns\n",
        "2. <font color='E3A440'><b>Metadata</b></font>: it is an information about an unstructured block of data (i.e., the author of a paper is a metadata of the text of that paper).\n",
        "3. <font color='E3A440'><b>Token</b></font>: each sequence of character which constitue an indipendent linguistic unit, that is a word (i.e., each occurence of the verb eat is a different token).\n",
        "4. <font color='E3A440'><b>Type</b></font>: The unique string of caracter which represent several occurrences of the same linguistic unit or word (i.e., the string 'eat' is the type for each token of the verb to eat).\n",
        "5. <font color='E3A440'><b>POS tag</b></font>:  a process which aims to assign parts of speech to each word of a given text\n",
        "6. <font color='E3A440'><b>Stopwords</b></font>: it refers to words that are very low semantic content such as article (the, an), modals (would, must), etc. \n",
        "7. <font color='E3A440'><b>Lemmatization</b></font>: a process that reduces the inflected words properly ensuring that the root word belongs to the language\n",
        "8. <font color='E3A440'><b>Document-Term matrix</b></font>: it is a matrix where rows represent segment of text and columns are filled by linguistic features of that segment of text.\n",
        "9. <font color='E3A440'><b>Weighting</b></font>: it is the weigth a mathematical function provides for each linguistic feature of each segment of text (i.e., Term frequency, Tf-Idf, BM25, etc.)\n",
        "10. <font color='E3A440'><b>K-means clustering</b></font>: an unsupervised statistic method aiming to partition *n* observations into *k* clusters based on their proximity to the centroid\n",
        "\n",
        "\n",
        "### Some basic concepts\n",
        "The main step in text mining is to convert unstructured textual data into a mathematical model to be used in statistical learning. Thus, we need to create a <font color='E3A440'>**Document-Term matrix**</font>, a matrix $n \\times w$, where $n$ is the number of text segments and $w$ is the number of textual features selected.The textual feature can have different nature. In the most simple model, these features correspond to the set of types that resume each token of the corpus. In other terms, $w$ is the number of features that characterize a segment of text The matrix is generally represented as follow:  \n",
        " \n",
        "$$X = \\begin{bmatrix} \n",
        "x_{11} & x_{12} & \\ldots & x_{1w} \\\\\n",
        "\\vdots & \\vdots       &  \\ddots      & \\vdots \\\\ \n",
        "x_{n1} & x_{12} & \\ldots & x_{nw} \\\\\n",
        "\\end{bmatrix}\n",
        "$$ \n",
        " \n",
        "\\\\\n",
        "When we apply a <font color='E3A440'>**clustering algorithm**</font> to this matrix, we want to group rows in a homogeneous set of clusters. This means to minimize the intra-class inertia or the pairwise squared deviations of points in the same cluster:\n",
        "\n",
        "$$ \\underset{s}{\\arg\\min}\\sum_{i=1}^{k}\\frac{1}{\\vert S_i\\vert}\\sum_\\limits{x,y \\in S_i}\\Vert x-y \\Vert^2$$\n",
        "\n",
        "This kind of algorithms aim to create a vector $Y$ of size $n$, containing the <font color='E3A440'>**cluster label**</font> assigned to each segment of text from $1$ to $k$.\n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix} \n",
        "c_1 \\\\\n",
        "c_2 \\\\\n",
        "\\vdots \\\\ \n",
        "c_n\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "Thus, $Y_1$ correspond to the cluster label given at $X_1$. Generally, $k$ is the main parameters of a clustering algorithm, which represent the number of cluster into which group text segments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRKQ8i7BzkB7"
      },
      "source": [
        "## 0.1 Preparation of Colab Virtual Machine\n",
        "\n",
        "In order to work correctly on Colab, we need to prepare the environment by executing two main steps:\n",
        "1. Download data from the GitHub project \n",
        "2. Install package to run code of this workshop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9mMQQWJzkB7"
      },
      "outputs": [],
      "source": [
        "# Downloading of data from the GitHub project\n",
        "!rm -rf TEST_PRECONFERENCE/\n",
        "!git clone https://github.com/puli83/TEST_PRECONFERENCE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation of packages\n",
        "!pip install pickle5\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "GHzhKocKltaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmzFMxo-zkB8"
      },
      "source": [
        "## 0.2 Import packages\n",
        "\n",
        "We import the following packages that contain the functions needed to accomplish our task: \n",
        "- `re` une librairie pour les expressions régulières\n",
        "- `matplotlib` pour tracer des graphiques\n",
        "- `numpy` pour des fonctions d'algèbre linéaire\n",
        "- `pandas` pour manipuler les données\n",
        "- `sklearn` (scikit-learn) pour des modèles d'apprentissage automatique ainsi que des fonctions reliées à la pratique de l'apprentissage automatique\n",
        "- `spacy` designed to build information extraction or natural language understanding systems. It supports several functions to performs several NLP task such us POS tag and NER\n",
        "- `nltk` include over 50 corpora, access to lexical resources such as WordNet and function to perform text processing task as tokenization, tagging and parsing "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy\n",
        "import datetime\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import pickle5 as pickle\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer #, TfidfVectorizer\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "tlI8PkH18Ofi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.3 Definition of custom functions\n",
        "In the next chunk of code, we define the functions that we need to reach the goal of the workshop. These functions will not be explained in details since their comprehencion is not essenctial to accomplish the workshop."
      ],
      "metadata": {
        "id": "KNlCfnZ79Ec5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data_for_WC(DTM, vocabulary_dtm):\n",
        "    # compute total frequency for each word\n",
        "    values_words = sum(DTM)\n",
        "    #values_words = sum(tfidf_matrix)\n",
        "    # verify type result and prepare data for wordcloud\n",
        "    if type(values_words) is np.ndarray:\n",
        "        values_words = [float(value) for value in np.nditer(values_words)]\n",
        "    elif type(values_words) is scipy.sparse.csr.csr_matrix:\n",
        "        values_words = [float(value) for value in np.nditer(values_words.todense())]\n",
        "    else:\n",
        "        print(\"Matrix in argument DTM has to be one of these two data classes:  'scipy.sparse.csr.csr_matrix' or 'numpy.ndarray'\")\n",
        "    ##\n",
        "    list_mots = sorted(vocabulary_dtm.items(), key= lambda x:x[1])\n",
        "    list_mots = [word for (word,idx) in  list_mots]\n",
        "    words = zip(list_mots, values_words)\n",
        "    words = sorted(words, key= lambda x:x[1], reverse=True)\n",
        "    ## prepare data structure for wordcloud\n",
        "    result_for_WC = {}\n",
        "    #iterating over the tuples lists\n",
        "    for (key, value) in words:\n",
        "        result_for_WC[key] = value\n",
        "    #\n",
        "    return result_for_WC\n",
        "\n",
        "def lexical_keyness(DTM, cls_kmeans, n_cluster = 0):\n",
        "    import math\n",
        "    # This code ref takes inspiration from this python module : https://pypi.org/project/corpus-toolkit/\n",
        "    # and its main script:  https://github.com/kristopherkyle/corpus_toolkit/blob/master/corpus_toolkit/corpus_tools.py\n",
        "    # which is based on this paper: https://aclanthology.org/J93-1003/\n",
        "\n",
        "    cluster_keyness = n_cluster\n",
        "    df_freq_target = pd.DataFrame(np.asarray(DTM[cls_kmeans.labels_ == cluster_keyness].sum(0).T).reshape(-1))#, columns = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]))\n",
        "    df_freq_target.index = [word for (word,idx) in sorted(vectorized.vocabulary_.items(), key= lambda x:x[1])]\n",
        "    df_freq_target.index\n",
        "    df_freq_target.columns = ['freq1']\n",
        "    df_freq_target['freq2'] = np.asarray(DTM[~(cls_kmeans.labels_ == cluster_keyness)].sum(0).T).reshape(-1)\n",
        "    #\n",
        "    df_freq_target['tot'] = df_freq_target['freq1'] + df_freq_target['freq2']\n",
        "    #\n",
        "    df_freq_target['freq1'] = df_freq_target['freq1'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    df_freq_target['freq2'] = df_freq_target['freq2'].apply(lambda x: 0.0000001 if x == 0 else x).astype(float)\n",
        "    #\n",
        "    df_freq_target['freq1_norm'] = df_freq_target['freq1']/df_freq_target['freq1'].sum() * 1000000\n",
        "    df_freq_target['freq2_norm'] = df_freq_target['freq2']/df_freq_target['freq2'].sum() * 1000000\n",
        "    #\n",
        "    df_freq_target['fraction'] = df_freq_target['freq1_norm'] / df_freq_target['freq2_norm']\n",
        "    df_freq_target['Log-likelihood Ratio'] = df_freq_target['fraction'].apply(math.log2)\n",
        "    return df_freq_target\n",
        "\n",
        "def Clustering_kmeans(DTM, reduce_lsa = False, n_comp_lsa= 300, n_clusters = 2,  **kwargs):\n",
        "    \"\"\"\n",
        "    Paramaters:\n",
        "    DTM;  Matrice to pass under sklearn classifiers\n",
        "    n_clusters = 2; the number of cluster to generate.\n",
        "    reduce_lsa = False; If True, a SVD is executed on the DTM before to execute the clustering\n",
        "    n_comp_lsa= 300; Number of component to keep after SVD on DTM.\n",
        "    **kwargs; Arguments for Kmeans method of sklearn.cluster. For exemple :\n",
        "    random_state = 1234,\n",
        "    max_iter = 300,\n",
        "    n_init = 20,\n",
        "    tol = .000001,\n",
        "    init = \"k-means++\",\n",
        "    n_jobs = -1,\n",
        "    precomputeDistances = True\n",
        "    \"\"\"\n",
        "    import datetime\n",
        "    from sklearn.cluster import KMeans\n",
        "    from sklearn import metrics\n",
        "    from sklearn.pipeline import make_pipeline\n",
        "    from sklearn.preprocessing import Normalizer\n",
        "    \n",
        "    ## dimensionality reduction\n",
        "    if reduce_lsa == True:\n",
        "        from sklearn.decomposition import TruncatedSVD\n",
        "        svd = TruncatedSVD(n_comp_lsa,\n",
        "                           n_iter = 100,\n",
        "                           random_state = kwargs['random_state'])\n",
        "        #\n",
        "        X_lsa = svd.fit_transform(DTM)\n",
        "        explained_variance = svd.explained_variance_ratio_.sum()\n",
        "        print(\"Explained variance of the SVD step: {}%\".format(\n",
        "            int(explained_variance * 100)))\n",
        "        #\n",
        "        DTM = X_lsa\n",
        "    # Normalize samples individually to unit norm. Scaling inputs to unit norms is a common operation for text classification or clustering for instance.\n",
        "    DTM = Normalizer().transform(DTM)\n",
        "    cls_kmeans = KMeans(n_clusters = n_clusters,\n",
        "                        random_state = kwargs['random_state'],\n",
        "                        max_iter = kwargs['max_iter'],\n",
        "                        n_init = kwargs['n_init'],\n",
        "                        tol = kwargs['tol'],\n",
        "                        init = kwargs['init'])\n",
        "    \n",
        "    cluster_labels = cls_kmeans.fit_predict(DTM)\n",
        "    ## silhouette\n",
        "    silhouette_avg = metrics.silhouette_score(DTM, cluster_labels)\n",
        "    print(\"For n_clusters = %i;\\t The silhouette_avg is : %f5\" %(n_clusters, silhouette_avg))\n",
        "    #\n",
        "    cls_kmeans.DTM_ = DTM\n",
        "    return cls_kmeans\n",
        "\n",
        "def wordcloud_par_cluster(wordcloud, DTM, cls_kmeans, vocab, first_n_words=10, figsize=(18, 16), fontsize=32, plot_wordcloud = True, lst_clust = [], title_in_plot = \"Clust_\"):\n",
        "\n",
        "        \"\"\"\n",
        "        wordcloud; A WordCloud function.\n",
        "        DTM; A Docuemnt-Term Matrix\n",
        "        vocab; It is a vocabulary from skllarn vectorizer\n",
        "        first_n_words = 10; How many words to print\n",
        "        figsize = (18, 16); Size of the plot. (this is the argument of this line plt.figure(figsize=figsize))\n",
        "        fontsize = 32; Size of title font\n",
        "        lst_clust = []; The list of cluster to plot. If empty, all the clusters are plotted\n",
        "        title_in_plot = \"Clust_\"; title to put on top of plot \\n\n",
        "        \"\"\"\n",
        "        import numpy\n",
        "        import scipy\n",
        "        \n",
        "        if not lst_clust:\n",
        "            lst_clust = list(range(cls_kmeans.n_clusters))\n",
        "\n",
        "        for x in lst_clust:\n",
        "            DTM_temp = DTM[cls_kmeans.labels_ == x]\n",
        "            result_for_WC= prepare_data_for_WC(DTM_temp, vocab)\n",
        "            ###\n",
        "            if plot_wordcloud == True:\n",
        "                plot = wordcloud.generate_from_frequencies(result_for_WC)\n",
        "                plt.figure(figsize=figsize)\n",
        "                plt.imshow(plot)\n",
        "                plt.title(title_in_plot + str(x) + '  N. of documents=' + str(DTM_temp.shape[0]),\n",
        "                        fontsize = fontsize,\n",
        "                        bbox=dict(facecolor='red', alpha=0.5))\n",
        "                plt.axis('off')\n",
        "                plt.show()\n",
        "            print(f\"Most frequent words for cluster {x} of size {str(DTM_temp.shape[0])} docs: \", list(result_for_WC)[0:first_n_words])\n",
        "\n",
        "def plot_data_by_cluster(DTM, cls_kmeans):\n",
        "    ## Reduction of dimension to 2 for visualisation reasons\n",
        "    from sklearn.manifold import TSNE\n",
        "    time_start = time.time()\n",
        "    tsne = TSNE(n_components=2, verbose=1, perplexity=30, n_iter=1000,metric='cosine', learning_rate=10, random_state = 794)\n",
        "    reduc_dim_results = tsne.fit_transform(DTM)\n",
        "    print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
        "\n",
        "    ## Create data structure for plotting\n",
        "    df_reduction = pd.DataFrame()\n",
        "    df_reduction['y'] =  cls_kmeans.labels_\n",
        "    df_reduction['1-dim'] = reduc_dim_results[:,0]\n",
        "    df_reduction['2-dim'] = reduc_dim_results[:,1]\n",
        "\n",
        "    ## Generate the plot\n",
        "    import seaborn as sns\n",
        "    import colorcet as cc\n",
        "    plt.figure(figsize=(16,10))\n",
        "    sns.scatterplot(data = df_reduction,\n",
        "                    x=\"1-dim\",\n",
        "                    y=\"2-dim\",\n",
        "                    hue=\"y\",\n",
        "                    palette = sns.color_palette(cc.glasbey, n_colors = cls_kmeans.n_clusters),)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ORZsrIP9lm7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.4 Import data\n",
        "\n"
      ],
      "metadata": {
        "id": "ebi993i_8XPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code sets the path from which to import data."
      ],
      "metadata": {
        "id": "CGayi-cExJSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT_DIR='TEST_PRECONFERENCE/'\n",
        "DATA_DIR=os.path.join(ROOT_DIR, 'Data')"
      ],
      "metadata": {
        "id": "E6Th_5mB8rAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We need to import the `.csv` file with the companies data. The data is imported in a tabular format (such as MS Excel) called **dataframe** and managed by `pandas`."
      ],
      "metadata": {
        "id": "Xsjk2gvXOmol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data = pd.read_csv(os.path.join(DATA_DIR, 'BCorp_companies_Web_Data.csv'))"
      ],
      "metadata": {
        "id": "OS5_Fn13MTMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The dataset has *1110* rows and *129* columns."
      ],
      "metadata": {
        "id": "SmzMy4Lf68rA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.shape"
      ],
      "metadata": {
        "id": "fxp4AwWh7Ffv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Further exploring some rows, we focus on two columns: \n",
        "\n",
        "+ **company_id** contains the domain of each BCorp companies under study\n",
        "+ **text_web_page** contains a combination of all those pages founded in Wayback Machine for each company."
      ],
      "metadata": {
        "id": "zj32jGwz7Min"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.head()"
      ],
      "metadata": {
        "id": "_n2JxRkq7Ypn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we give a list of some important metadata to explore"
      ],
      "metadata": {
        "id": "cTlRTjB3R6Fr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data[['website', 'company_name', 'date_certified', 'description', 'industry','sector',\n",
        "       'country', 'state', 'city', 'assessment_year', 'overall_score']]\n"
      ],
      "metadata": {
        "id": "-a-NYE6ZSIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <font size = '6' color='E3A440'>Section 1: Data pre-processing </font>\n",
        "This section declines the main aspects of a classical preprocessing step of textual data. The main objective of this step is to organize unstructured data into a structure that can be digestible in a statistical learning process. In a few words, this process allows to transform the text into vectors and is divided into three main operations:\n",
        " 1. Morphological Analysis\n",
        " 2. Filter of lexical features\n",
        " 3. Vectorisation of lexical features"
      ],
      "metadata": {
        "id": "4CJuakgp8BhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Cleaning of metadata\n",
        "One thing to be aware when working with  real world data is that they are always \"dirty\".\n",
        "\n",
        "In this section, we provide a data cleaning step for our dataset. Since the state of Québec is written on two forms, one with accent and one without it, this issue generates two values for the same data.\n",
        "Thus, we replace each \"é\"  with a simple \"e\"."
      ],
      "metadata": {
        "id": "gD6_woCdorMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data['state'] = df_data['state'].str.replace(\"é\",\"e\")"
      ],
      "metadata": {
        "id": "h7YZLu4goyaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Morphological Analysis\n",
        "An important operation during the preprocessing of unstructured textual data is to detect the **Morphological features** of words. This allows understanding of the roles that words have in their contexts. This operation is composed of two parts:\n",
        "1. Part-of-Speech tagging, generally known as POS tagging\n",
        "2. Lemmatisation, which consists in the reduction of a word to his lemma\n",
        "\n",
        "These two sub-operations can be executed by different types of algorithms. We use a neural network pre-trained model using the module named `spacy`. In particular, we use the function `nlp.pipe()` to generate a list of preprocessed documents. This function has several arguments and it takes a list of string characters to be executed. Each element of this list is a textual segment or a document. "
      ],
      "metadata": {
        "id": "CfUdgcOHzFV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this first chunk of code, we load from `spacy` the pretrained model  called <font color=\"#CE9178\">en_core_web_sm</font> ([More info](https://spacy.io/models/en)).   "
      ],
      "metadata": {
        "id": "GNOg27M0jUiX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "hcxxLkyh-4Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In this second chunk, we apply the NLP pipeline to our textual data contained in the **text_combined** column, and this, by using the `nlp.pipe()` function. \n",
        "\n",
        "Thus, we generate the `preprocessed_docs` variable, namely a list containing preprocessed text of all the documents. We can observe the first document with `preprocessed_docs[0]`, the second with `preprocessed_docs[1]`, the third with `preprocessed_docs[2]`, etc.\n",
        "\n",
        "Each element of the list `preprocessed_docs`, contains a list of word analyzed with `spacy`.\n"
      ],
      "metadata": {
        "id": "B9SKcfPxPvpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = datetime.datetime.now() # line of code to register a timestamp \n",
        "df_data[\"text_preprocessed\"] = list(nlp.pipe(df_data[\"text_web_page\"], disable = [\"tok2vec\",'parser','ner']))\n",
        "print(str(datetime.datetime.now() - t0)) # line of code to print the elapsed time from t0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3d759d7-3c99-4d15-f9af-d3a1adbc0215",
        "id": "rVAjPpkiUgAG"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0:00:15.984149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk, we look at the first document in his **original format**"
      ],
      "metadata": {
        "id": "LrzGwAk0_0vh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print first segment of the corpus\n",
        "print(df_data.iloc[0][\"text_web_page\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeiIr4ECXX82",
        "outputId": "e9e76758-165e-4cd6-994f-b1768fadf6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explore how you can make every B2B purchase count towards UN's Sustainable Development Goals (SDGs). 17 Ways connects SDG impact companies and multinationals for purpose-driven purchasing. Post a need and choose suppliers based on the products or services they offer and on the SDG areas you want to impact. You can also choose to search/sort by certifications. Appropriate companies will be alerted about your opportunity, but your information will be kept secure so you can decide how to engage and you won't be overwhelmed by potential suppliers. Now it's easy to be found by companies looking to purchase from businesses that are aligned with the SDG areas your company is working to impact. And 17 Ways helps you connect with other like-minded companies to create partnerships and do business together. The UN's Sustainable Development Goals (SDGs) outline 17 Ways companies can contribute to a sustainable planet and society. All the companies on 17 Ways choose which of the SDGs align with their mission. Many companies are additionally identified by third party certifications; we are actively working with these certifiers to develop measurement and reporting tools. Explore our marketplace filled with 17 Ways to make an impact with every purchase. Fill out this simple form Get ready to receive applications from users matching your request.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we look at the **preprocessed version** of this first document. In particular, we print three attributes for each word of the first document:\n",
        "1. `word.text`, which corresponds to the original version of the word, which is called **token**.\n",
        "2. `word.pos_`, which corresponds to the POS tag predicted for that word.\n",
        "3. `word.lemma_`, which corresponds to the lemma of the token.\n",
        "\n",
        "For padagogical reasons, we only look at the first 10 words.\n"
      ],
      "metadata": {
        "id": "igbdW_Pc_8hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print morphosyntactial analysis of the first sentence of the corpus. Each element is separeted by a vertitcal line |\n",
        "for idx, word in enumerate(df_data[\"text_preprocessed\"].iloc[0]):\n",
        "    print(\"Token: \", word.text, \" | \", \"POS tag: \", word.pos_,\" | \", \"Lemma of the token: \", word.lemma_)\n",
        "    # Break loop after first 10 words -> idx==10\n",
        "    if idx == 10:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXZrHo9ETUiv",
        "outputId": "c121f740-2311-4846-b768-98edd4b09629"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token:  Explore  |  POS tag:  VERB  |  Lemma of the token:  explore\n",
            "Token:  how  |  POS tag:  ADV  |  Lemma of the token:  how\n",
            "Token:  you  |  POS tag:  PRON  |  Lemma of the token:  -PRON-\n",
            "Token:  can  |  POS tag:  VERB  |  Lemma of the token:  can\n",
            "Token:  make  |  POS tag:  VERB  |  Lemma of the token:  make\n",
            "Token:  every  |  POS tag:  DET  |  Lemma of the token:  every\n",
            "Token:  B2B  |  POS tag:  NOUN  |  Lemma of the token:  b2b\n",
            "Token:  purchase  |  POS tag:  NOUN  |  Lemma of the token:  purchase\n",
            "Token:  count  |  POS tag:  NOUN  |  Lemma of the token:  count\n",
            "Token:  towards  |  POS tag:  ADP  |  Lemma of the token:  towards\n",
            "Token:  UN  |  POS tag:  PROPN  |  Lemma of the token:  UN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 Filter of lexical features\n",
        "\n",
        "In this section, we perform several pre-processing steps to transform the text to a format that can be understood and analyzed by our methods. \n",
        "1.   Eliminating stopwords : removing the low-level information from our text in order to give more focus to the important information\n",
        "2.   Keeping only adverb, adjective, noun and verb\n"
      ],
      "metadata": {
        "id": "DW_cSk1TAayz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following chunck, we load the list of english **stopwords** from the package `nltk`. Since the list is customizable, we add the verb \"would\" that is not among the modale verbs included in this list. "
      ],
      "metadata": {
        "id": "a0Y3jeyUg3X6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download default list of stopword\n",
        "nltk.download('stopwords')\n",
        "# add custum words to the stopword list\n",
        "stopwords_list = set(stopwords.words('english') + ['would'])"
      ],
      "metadata": {
        "id": "W7TCJHpej-RP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96e387d6-bb73-4120-c9e9-9b1e9b890695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the following chunk of code, we will use the previous the list of stopwords to filter words and the POS tag analysis to select only the words that are adverbs, adjectives, nouns and verbs to further highlight the part of speach meaningfull for our task."
      ],
      "metadata": {
        "id": "fiIwdtOEdfGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, row in df_data.iterrows():\n",
        "    print(f\"Sentence n. {idx}\")\n",
        "    print(\"Original :\\t \",row['text_web_page'])\n",
        "    print(\"Preprocessed:\\t \", [w.lemma_.lower() for w in row['text_preprocessed'] if w.pos_ in [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"]  and w.text.lower() not in stopwords_list])\n",
        "    if idx == 5:\n",
        "        break"
      ],
      "metadata": {
        "id": "qgVCsYwTAX8C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7304360b-db39-45ab-d7e4-5ad6a147180b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence n. 0\n",
            "Original :\t  Explore how you can make every B2B purchase count towards UN's Sustainable Development Goals (SDGs). 17 Ways connects SDG impact companies and multinationals for purpose-driven purchasing. Post a need and choose suppliers based on the products or services they offer and on the SDG areas you want to impact. You can also choose to search/sort by certifications. Appropriate companies will be alerted about your opportunity, but your information will be kept secure so you can decide how to engage and you won't be overwhelmed by potential suppliers. Now it's easy to be found by companies looking to purchase from businesses that are aligned with the SDG areas your company is working to impact. And 17 Ways helps you connect with other like-minded companies to create partnerships and do business together. The UN's Sustainable Development Goals (SDGs) outline 17 Ways companies can contribute to a sustainable planet and society. All the companies on 17 Ways choose which of the SDGs align with their mission. Many companies are additionally identified by third party certifications; we are actively working with these certifiers to develop measurement and reporting tools. Explore our marketplace filled with 17 Ways to make an impact with every purchase. Fill out this simple form Get ready to receive applications from users matching your request.\n",
            "Preprocessed:\t  ['explore', 'make', 'b2b', 'purchase', 'count', 'connect', 'impact', 'company', 'multinational', 'purpose', 'drive', 'purchasing', 'post', 'need', 'choose', 'supplier', 'base', 'product', 'service', 'offer', 'area', 'want', 'impact', 'also', 'choose', 'search', 'sort', 'certification', 'appropriate', 'company', 'alert', 'opportunity', 'information', 'keep', 'secure', 'decide', 'engage', 'will', 'overwhelm', 'potential', 'supplier', 'easy', 'find', 'company', 'look', 'purchase', 'business', 'align', 'area', 'company', 'work', 'impact', 'help', 'connect', 'like', 'minded', 'company', 'create', 'partnership', 'business', 'together', 'outline', 'company', 'contribute', 'sustainable', 'planet', 'society', 'company', 'way', 'choose', 'sdg', 'align', 'mission', 'many', 'company', 'additionally', 'identify', 'third', 'party', 'certification', 'actively', 'work', 'certifier', 'develop', 'measurement', 'reporting', 'tool', 'explore', 'marketplace', 'fill', 'way', 'make', 'impact', 'purchase', 'fill', 'simple', 'form', 'ready', 'receive', 'application', 'user', 'match', 'request']\n",
            "Sentence n. 1\n",
            "Original :\t  We're a parent company founded on a promise to make home better. Not just your home, but our home — the world we all share. So show us your dirtiest, hungriest jobs — we're up to the task. Together, we can make our home better. One room, one brand at a time. We remember when you had to choose between cleaning performance and a non-toxic home. Those days are long gone. Get your hands (and clothes and pans and floors) dirty. These impressively powerful, naturally clean products will tackle any mess. We know the kitchen is the heart of your home. Let us help you stock it with the freshest, healthiest, and most delicious ingredients. These tasty sauces are fresh ingredients meets old world flavor in one convenient jar. We believe snacks should do more than check the boxes, they should pass the \"can I have another one?\" test, too. From fertility and pregnancy to nursing and beyond, our products are crafted by food lovers and backed by experts to keep you safe and satisfied through every stage of motherhood. A champion of better for all, we were frustrated by the lack of yummy snacks for special diets. Then we found these rockstar goodies. Snacks are always too something. You get the idea. Finally, a snack that's just right.\n",
            "Preprocessed:\t  ['parent', 'company', 'found', 'promise', 'make', 'home', 'better', 'home', 'home', 'world', 'share', 'show', 'dirty', 'hungry', 'job', 'task', 'together', 'make', 'home', 'better', 'room', 'brand', 'time', 'remember', 'choose', 'clean', 'performance', 'non', '-', 'toxic', 'home', 'day', 'long', 'go', 'hand', 'clothe', 'pan', 'floor', 'dirty', 'impressively', 'powerful', 'naturally', 'clean', 'product', 'tackle', 'mess', 'know', 'kitchen', 'heart', 'home', 'let', 'help', 'stock', 'fresh', 'healthy', 'delicious', 'ingredient', 'tasty', 'sauce', 'fresh', 'ingredient', 'meet', 'old', 'world', 'flavor', 'convenient', 'jar', 'believe', 'snack', 'check', 'box', 'pass', 'one', 'test', 'fertility', 'pregnancy', 'nursing', 'beyond', 'product', 'craft', 'food', 'lover', 'back', 'expert', 'keep', 'safe', 'satisfied', 'stage', 'motherhood', 'champion', 'well', 'frustrate', 'lack', 'yummy', 'snack', 'special', 'diet', 'find', 'rockstar', 'goody', 'snack', 'always', 'get', 'idea', 'finally', 'snack', 'right']\n",
            "Sentence n. 2\n",
            "Original :\t  Opening Date: August 2019 Address: 1921 8th St NW #115, Washington,... Location: 98 District Square SW, Washington, DC 20024 Open Date:February 2018... Location: 1221 Van Street SE Setting: Within feet of the D.C.'s... Location: 4121 Wilson BLVD Suite 102B, Arlington, VA 22203 Project Type:... Address: 901 North Saint Asaph Street, Alexandria, VA, 22314 Seat Count,... Location: 1837 M Street NW, Washington D.C. Location: 1401 New York Avenue NW, Washington DC 20005 Setting: The... Location: Columbia, South Carolina Date project completed: Summer 2018 Photography: The Rohm Group Project... Location: 82 I Street SE, Washington, DC 20003 Completed: Spring, 2017 Project... Location: Annapolis, Maryland Completed: Summer 2018 Project Description: An ambitious renovation project by... The result of a collaboration between D.C.-based architecture and interior design... Tasked with repurposing Washington DC's historic Equitable Bank Building and transforming... Location: White Marsh, MD Date project completed: Fall 2017 Total size: 3,650 sq. ft. and... Location: Potomac, Maryland Date project completed: Fall 2017 Project Description: Residence7801 is a... Location: Fort Worth, Texas Date project completed: Fall 2016 Project Description: Interior design for... Location: Virginia Beach, Virginia Date project completed: Winter 2016 Project Description: Full Renovation... Location: Charleston, South Carolina Date project completed: Fall 2016 Interior Design: P3 Design Collective... Location: Washington, DC Date project completed: Winter 2017 Total square footage: 7,930 sq. ft. PACERS RUNNING CLARENDON //3877 worked hand in hand with Pacers to bring in elements... Location: 1090 I St NW, Washington, DC 20001 Owner: Momofuku CCDC... Location: Washington, DC Owner: Danielle Vogel Date project completed: December 2015 Total square footage: Photographer: Ronald... Location: Washington, DC Date project completed: Spring 2016 Project Description: Phoenix Park Hotel, Washington,... Project Location: Washington, DC Date project completed: Spring 2015 Project Description: The Randolph... Location: Denver, CO Owner: AppleREIT Total Square Footage: 3,850 sq. ft. Location: Fairfax, VA Owner: AppleREIT Date project completed: Fall 2014 Total square footage: 40,000 sq.... Location: Washington, DC Owner: Anne Mahlum Date project completed: Spring 2014 Total square... Project Location: Reston, VA Date project completed: Spring 2010 Project Description:... Location: 1901 14th Street NW Owner: Matchbox Food Group Date project completed: November... Location: Various European Locations Owner: Marriott Project Description: Moxy is a new economy-focused... Location: Washington, DC Owner: 3 Stars Brewing Date project completed: Fall 2015... Project Location: Washington, DC Date project completed: Spring 2010 Project Description: Residence156... Location: New York, NY Owner: The Atwood Kitchen + Bar Room... Project Location: Gaithersburg, MD Owner: Baywood Hotels Date project completed: Spring... Project Location: Washington, DC Date project completed: Spring 2015 Project Description: 2917... Project Location: Washington, DC Date project completed: Fall 2010 Project Description: In... Location: Various United States Locations Owner: Marriott Project Description: //3877 was tasked with... Project Location: Washington, DC Project Description: Mansion History The Patterson Mansion... Restaurant Location: Washington, DC Date project completed: Fall 2012 Photographer: Daniel... Project Location: College Park, MD Owner: Baywood Hotels Services Provided: Architecture... Location: Arlington, VA Owner: Anne Mahlum Date project completed: Summer 2014 Total... Project Location: Washington, DC Developer: Property Alchemist Date project completed: Fall 2014 Millwork: Cabinet... Location: Alexandria, VA Owner: Carluccio's Date project completed: Fall 2014 Total... Project Location: Washington, DC Owner: Sundaram Development Date project completed: Fall 2013... Location: Washington, DC Owner: Anne Mahlum Date project completed: Fall 2013 Total square... Location: Washington, DC Owner: Danielle Vogel Date project completed: April 2013 Total square... Restaurant Location: Washington, DC Restaurant Type: Sit Down Dining and Retail... Location: Washington, DC Owner: Capital City Care Date project completed: Spring 2013... Easy is not to be underestimated. Design is not just a visual thing, it's a thought process, a skill. Ultimately design is a tool to enhance our humanity. You were once wild here. Never half-ass two things. When you have a choice between being right and being kind, choose kind. Dr. Work hard & be nice to people. The Results \"//3877 has the creativity to make every space a show place and the architectural depth to make every set a plans a well thought out, buildable design\" \"//3877 has the creativity to make every space a show place and the architectural depth to make every set a plans a well thought out, buildable design\" \"To say that working with //3877 was an outstanding experience is almost an understatement. The cooperation and sensitivity that David Shove-Brown and Ryan Petyak exhibited on our project and towards our concerns was exemplary. As skilled listeners, they invested the time to learn our project goals and then execute that vision in their design and project management. I sincerely believe that the phenomenal success of our project is largely attributed to their attention and efforts. Additionally, they made the work fun. They are a dynamite team that I thoroughly enjoyed working with and I only wish that I could clone them for future projects in other cities!\" \"//3877's vision and execution provides for the perfect balance of cutting edge contemporary while maintaining the character of historic details. They have become the cornerstone partner for my commercial and residential development firm because of that unique and talented approach.\" \"//3877's vision and execution provides for the perfect balance of cutting edge contemporary while maintaining the character of historic details. They have become the cornerstone partner for my commercial and residential development firm because of that unique and talented approach.\" \"We've been working with //3877 for a number of years now on numerous projects and have found David and his team to be a pleasure to work with. I recommend them often and without reservation.\" \"We've been working with //3877 for a number of years now on numerous projects and have found David and his team to be a pleasure to work with. I recommend them often and without reservation.\" \"//3877 is so more than an architecture firm. Their designs are brilliant, cost effective, and beautiful. Their team is incredibly attentive and responsive and they make every effort to ensure that the daunting task of completing a build out is as simple and easy as possible.\" \"//3877 is so more than an architecture firm. Their team is incredibly attentive and responsive and they make every effort to ensure that the daunting task of completing a build out is as simple and easy as possible.\" \"It has been my pleasure to have //3877 as our Architect for the past 2 years. I could not be happier. In my forty years of design and construction I have never seen a more talented and caring design team. The skills they bring ensure the highest level of design and quality. I am particularly impressed with the thought that went into the build and co-ordination with the client, design team and contractor. What began as another Architect – Owner relationship soon developed into partnership and friendship. I look forward to our partnership as we move down the Eastern Seaboard, South and South West with new Veterinary Hospitals.\n",
            "Preprocessed:\t  ['opening', 'date', 'address', '8th', 'location', 'open', 'location', 'setting', 'foot', 'location', 'address', 'location', 'set', 'location', 'project', 'complete', 'summer', 'location', 'spring', 'location', 'complete', 'ambitious', 'renovation', 'project', 'result', 'collaboration', 'd.c.-based', 'architecture', 'interior', 'design', 'task', 'repurpose', 'historic', 'transforming', 'location', 'project', 'complete', 'fall', 'total', 'size', 'location', 'project', 'complete', 'fall', 'description', 'location', 'project', 'complete', 'fall', 'interior', 'design', 'location', 'project', 'complete', 'winter', 'full', 'renovation', 'location', 'project', 'complete', 'fall', 'location', 'project', 'complete', 'winter', 'total', 'square', 'footage', 'pacer', 'work', 'hand', 'hand', 'pacer', 'bring', 'element', 'location', 'owner', 'momofuku', 'location', 'project', 'complete', 'total', 'square', 'footage', 'photographer', 'location', 'project', 'complete', 'spring', 'project', 'complete', 'spring', 'location', 'location', 'project', 'complete', 'fall', 'total', 'square', 'footage', 'location', 'project', 'complete', 'spring', 'total', 'square', 'project', 'complete', 'spring', 'location', 'project', 'complete', 'location', 'various', 'new', 'economy', 'focus', 'location', 'project', 'complete', 'fall', 'project', 'complete', 'spring', 'location', 'project', 'complete', 'spring', 'project', 'complete', 'spring', 'project', 'complete', 'fall', 'location', 'task', 'mansion', 'history', 'project', 'complete', 'fall', 'photographer', 'provide', 'architecture', 'location', 'project', 'complete', 'summer', 'total', 'developer', 'project', 'complete', 'fall', 'location', 'project', 'complete', 'fall', 'total', 'project', 'complete', 'fall', 'location', 'project', 'complete', 'fall', 'total', 'square', 'location', 'project', 'complete', 'total', 'square', 'sit', 'dining', 'location', 'project', 'complete', 'spring', 'easy', 'underestimate', 'design', 'visual', 'thing', 'thought', 'process', 'skill', 'ultimately', 'design', 'tool', 'enhance', 'humanity', 'wild', 'never', 'half', 'ass', 'thing', 'choice', 'right', 'kind', 'choose', 'kind', 'hard', 'nice', 'people', 'result', 'creativity', 'make', 'space', 'show', 'place', 'architectural', 'depth', 'make', 'set', 'plan', 'well', 'think', 'buildable', 'design', 'creativity', 'make', 'space', 'show', 'place', 'architectural', 'depth', 'make', 'set', 'plan', 'well', 'think', 'buildable', 'design', 'say', 'work', 'outstanding', 'experience', 'almost', 'understatement', 'cooperation', 'sensitivity', 'exhibit', 'project', 'concern', 'exemplary', 'skilled', 'listener', 'invest', 'time', 'learn', 'project', 'goal', 'execute', 'vision', 'design', 'project', 'management', 'sincerely', 'believe', 'phenomenal', 'success', 'project', 'largely', 'attribute', 'attention', 'effort', 'additionally', 'make', 'work', 'fun', 'dynamite', 'team', 'thoroughly', 'enjoy', 'work', 'wish', 'could', 'clone', 'future', 'project', 'city', 'vision', 'execution', 'provide', 'perfect', 'balance', 'cut', 'edge', 'contemporary', 'maintain', 'character', 'historic', 'detail', 'become', 'cornerstone', 'partner', 'commercial', 'residential', 'development', 'firm', 'unique', 'talented', 'approach', 'vision', 'execution', 'provide', 'perfect', 'balance', 'cut', 'edge', 'contemporary', 'maintain', 'character', 'historic', 'detail', 'become', 'cornerstone', 'partner', 'commercial', 'residential', 'development', 'firm', 'unique', 'talented', 'approach', 'work', 'number', 'year', 'numerous', 'project', 'find', 'team', 'pleasure', 'work', 'recommend', 'often', 'reservation', 'work', 'number', 'year', 'numerous', 'project', 'find', 'team', 'pleasure', 'work', 'recommend', 'often', 'reservation', 'architecture', 'firm', 'design', 'brilliant', 'cost', 'effective', 'beautiful', 'team', 'incredibly', 'attentive', 'responsive', 'make', 'effort', 'ensure', 'daunting', 'task', 'complete', 'build', 'simple', 'easy', 'possible', 'architecture', 'firm', 'team', 'incredibly', 'attentive', 'responsive', 'make', 'effort', 'ensure', 'daunting', 'task', 'complete', 'build', 'simple', 'easy', 'possible', 'pleasure', 'architect', 'past', 'year', 'could', 'happy', 'year', 'design', 'construction', 'never', 'see', 'talented', 'care', 'design', 'team', 'skill', 'bring', 'ensure', 'high', 'level', 'design', 'quality', 'particularly', 'impressed', 'thought', 'go', 'build', 'co', '-', 'ordination', 'client', 'design', 'team', 'contractor', 'begin', 'architect', 'owner', 'relationship', 'soon', 'develop', 'partnership', 'friendship', 'look', 'forward', 'partnership', 'move', 'new']\n",
            "Sentence n. 3\n",
            "Original :\t  Making urgent corporate utility global bold climate action possible 3Degrees helps organizations around the world implement renewable energy, transportation decarbonization, and other climate solutions. How Verisk is Successfully Addressing Emissions from its International Energy Load 3Degrees crafted a cost-competitive plan for Verisk to address its global greenhouse gas emissions through a diverse portfolio of high-quality energy attribute certificates (EACs) and carbon offsets. Tackling the Decarbonization of Transportation As the largest emitter of greenhouse gases in the U.S., the transportation sector is exactly where we need to focus our collective efforts to reduce emissions. The solution that will have the biggest impact: transitioning to low-carbon transportation fuels. How Mondelez International is Reducing its Global GHG Emissions Mondelēz International enlisted 3Degrees to develop a global strategy roadmap and implement a new VPPA to help the company meet its ambitious emissions reduction goal. Companies Partner on Largest Renewable Energy Aggregation to Date 3Degrees helped four leading companies leverage their collective buying power in an innovative renewable energy aggregation, demonstrating how corporate energy buyers of all sizes can successfully purchase renewable energy directly from new renewable projects. Maximizing the Value of Your Green Power Programs Fully optimized green power programs provide a huge opportunity for utilities. When designed and implemented well, these programs offer numerous benefits. Invest in verified carbon offset projects Take immediate action on your Scope 1 (direct) and Scope 3 (indirect) GHG emissions, including transportation emissions, by investing in carbon offset projects. Support renewable energy on a local and global scale Reduce the environmental impact of your global energy use through high-quality, country-specific instruments like RECs, GOs, I-RECs, and TIGRs. Meet your emissions reduction and climate goals Whether you are just getting started on your sustainability journey or are a sophisticated energy buyer, meet your climate and renewable energy goals with customized, best-fit solutions. Offer utility voluntary renewable energy programs Create value for your utility and your customers with a suite of well-designed voluntary programs, including green power, community solar, green tariff, and renewable natural gas programs. Reduce the impact of transportation emissions Address the impact of your transportation-related emissions with a customized program of decarbonization solutions that incorporates Low Carbon Fuel Standard (LCFS) support, fleet decarbonization consulting, and carbon offset project investment within the transportation sector. Etsy has a long-standing track record of working to reduce our environmental footprint. Our carbon neutral shipping initiative is a major milestone within our larger commitment to act urgently and aggressively in the fight against climate change. The signing of this VPPA is a major advancement towards achieving our aggressive global emissions reduction goals. We have worked hand-in-hand with the 3Degrees team for more than a decade; in that time they have helped us make our Blue Sky renewable energy program one of the top renewable energy programs in the country. 3Degrees' expertise in program design and implementation has allowed us to continually evolve the Blue Sky program to keep it relevant to customers, the renewable energy market, and to the utility. We are updating our events calendar. Please check back soon.\n",
            "Preprocessed:\t  ['make', 'urgent', 'corporate', 'utility', 'global', 'bold', 'climate', 'action', 'possible', 'help', 'organization', 'world', 'implement', 'renewable', 'energy', 'transportation', 'decarbonization', 'climate', 'solution', 'successfully', 'address', 'emission', 'craft', 'cost', 'competitive', 'plan', 'address', 'global', 'greenhouse', 'gas', 'emission', 'diverse', 'portfolio', 'high', 'quality', 'energy', 'attribute', 'certificate', 'carbon', 'offset', 'tackle', 'decarbonization', 'large', 'emitter', 'greenhouse', 'gas', 'transportation', 'sector', 'exactly', 'need', 'focus', 'collective', 'effort', 'reduce', 'emission', 'solution', 'big', 'impact', 'transition', 'low', 'carbon', 'transportation', 'fuel', 'reduce', 'global', 'enlist', 'develop', 'global', 'strategy', 'roadmap', 'implement', 'new', 'help', 'company', 'meet', 'ambitious', 'emission', 'reduction', 'goal', 'company', 'large', 'renewable', 'energy', 'help', 'lead', 'company', 'leverage', 'collective', 'buying', 'power', 'innovative', 'renewable', 'energy', 'aggregation', 'demonstrate', 'corporate', 'energy', 'buyer', 'size', 'successfully', 'purchase', 'renewable', 'energy', 'directly', 'new', 'renewable', 'project', 'maximize', 'value', 'fully', 'optimize', 'green', 'power', 'program', 'provide', 'huge', 'opportunity', 'utility', 'design', 'implement', 'well', 'program', 'offer', 'numerous', 'benefit', 'invest', 'verified', 'carbon', 'offset', 'project', 'take', 'immediate', 'action', 'scope', 'direct', 'indirect', 'emission', 'include', 'transportation', 'emission', 'invest', 'carbon', 'offset', 'project', 'support', 'renewable', 'energy', 'local', 'global', 'scale', 'reduce', 'environmental', 'impact', 'global', 'energy', 'use', 'high', 'quality', 'country', 'specific', 'instrument', 'rec', 'go', 'meet', 'emission', 'reduction', 'climate', 'goal', 'get', 'start', 'sustainability', 'journey', 'sophisticated', 'energy', 'buyer', 'meet', 'climate', 'renewable', 'energy', 'goal', 'customized', 'best', 'fit', 'solution', 'offer', 'utility', 'voluntary', 'renewable', 'energy', 'program', 'create', 'value', 'utility', 'customer', 'suite', 'well', 'design', 'voluntary', 'program', 'include', 'green', 'power', 'community', 'solar', 'green', 'tariff', 'renewable', 'natural', 'gas', 'program', 'reduce', 'impact', 'transportation', 'emission', 'address', 'impact', 'transportation', 'relate', 'emission', 'customized', 'program', 'decarbonization', 'solution', 'incorporate', 'support', 'fleet', 'decarbonization', 'consulting', 'carbon', 'offset', 'project', 'investment', 'transportation', 'sector', 'long', 'stand', 'track', 'record', 'work', 'reduce', 'environmental', 'footprint', 'carbon', 'neutral', 'shipping', 'initiative', 'major', 'milestone', 'large', 'commitment', 'act', 'urgently', 'aggressively', 'fight', 'climate', 'change', 'signing', 'major', 'advancement', 'achieve', 'aggressive', 'global', 'emission', 'reduction', 'goal', 'work', 'hand', 'hand', 'team', 'decade', 'time', 'help', 'make', 'renewable', 'energy', 'program', 'top', 'renewable', 'energy', 'program', 'country', 'expertise', 'program', 'design', 'implementation', 'allow', 'continually', 'evolve', 'program', 'keep', 'relevant', 'customer', 'renewable', 'energy', 'market', 'utility', 'update', 'event', 'calendar', 'check', 'back', 'soon']\n",
            "Sentence n. 4\n",
            "Original :\t  Near-time multidimensional analysis and intelligence gathering to quickly map safe routes, recovery and risk mitigation. The Intelligent Eye 3D Planeta's patented technology fuses 2D satellite and aerial images of the earth to create volumetric 3D images. Imagine a digital map, but instead of seeing the earth in 2D or simulated 3D, you saw an actual 3D image. That's 3D Planeta's starting point. We contribute at least one percent of annual sales to local environmental causes as part of this global alliance of businesses financially committed to creating a healthy planet. We are actively working on three of the 17 SDGs, the global call to action to support greener, more inclusive economies, and stronger, more resilient societies. Enter your email: Invalid value © 2020 3D Planeta. Necessary cookies are absolutely essential for the website to function properly. This category only includes cookies that ensures basic functionalities and security features of the website. These cookies do not store any personal information. Any cookies that may not be particularly necessary for the website to function and is used specifically to collect user personal data via analytics, ads, other embedded contents are termed as non-necessary cookies. It is mandatory to procure user consent prior to running these cookies on your website.\n",
            "Preprocessed:\t  ['near', 'time', 'multidimensional', 'analysis', 'intelligence', 'gathering', 'quickly', 'map', 'safe', 'route', 'recovery', 'risk', 'mitigation', '3d', 'patented', 'technology', 'fuse', 'satellite', 'aerial', 'image', 'earth', 'create', 'volumetric', '3d', 'image', 'imagine', 'digital', 'map', 'instead', 'see', 'earth', '2d', 'simulate', '3d', 'see', 'actual', '3d', 'image', '3d', 'starting', 'point', 'contribute', 'least', 'percent', 'annual', 'sale', 'local', 'environmental', 'cause', 'part', 'global', 'alliance', 'business', 'financially', 'commit', 'create', 'healthy', 'planet', 'actively', 'work', 'sdg', 'global', 'call', 'action', 'support', 'inclusive', 'economy', 'strong', 'resilient', 'society', 'enter', 'email', 'invalid', 'value', '3d', 'necessary', 'cookie', 'absolutely', 'essential', 'website', 'function', 'properly', 'category', 'include', 'cookie', 'ensure', 'basic', 'functionality', 'security', 'feature', 'website', 'cookie', 'store', 'personal', 'information', 'cookie', 'may', 'particularly', 'necessary', 'website', 'function', 'use', 'specifically', 'collect', 'user', 'personal', 'datum', 'analytic', 'ad', 'embed', 'content', 'term', 'non', '-', 'necessary', 'cookie', 'mandatory', 'procure', 'user', 'consent', 'prior', 'run', 'cookie', 'website']\n",
            "Sentence n. 5\n",
            "Original :\t  WHAT WE DO 3 Phases Renewables is a leading supplier of comprehensive renewable energy solutions. DIRECT ACCESS 3 Phases Renewables allows you to choose your renewable energy content at rates that are cost-competitive with your current utility's. DISTRIBUTED GENERATION 3 Phases Renewables is experienced in the design, financing, construction and operation of a variety of types of On-site generation systems. RENEWABLE ENERGY CREDITS 3 Phases Renewables purchases and sells Green-e certified RECs. Our REC program follows strict auditable procedures so that you can be assured that your REC purchase will legitimately further your environmental initiatives. By purchasing renewable power via Direct Access, the City avoids nearly 18,000 metric tons of carbon. Our commitment to sustainability starts at the workplace, but our greatest impact is achieved through the services we offer. Mike Mazur, Founder 3 Phases Renewables tops the list with the highest percentage of renewable power delivered.\n",
            "Preprocessed:\t  ['leading', 'supplier', 'comprehensive', 'renewable', 'energy', 'solution', 'direct', 'acces', 'allow', 'choose', 'renewable', 'energy', 'content', 'rate', 'cost', 'competitive', 'current', 'utility', 'distribute', 'generation', 'experience', 'design', 'financing', 'construction', 'operation', 'variety', 'type', 'site', 'generation', 'system', 'credit', 'phase', 'purchase', 'sell', 'e', 'certify', 'rec', 'program', 'follow', 'strict', 'auditable', 'procedure', 'assure', 'purchase', 'legitimately', 'environmental', 'initiative', 'purchase', 'renewable', 'power', 'avoid', 'nearly', 'metric', 'ton', 'carbon', 'commitment', 'sustainability', 'start', 'workplace', 'great', 'impact', 'achieve', 'service', 'offer', 'top', 'list', 'high', 'percentage', 'renewable', 'power', 'deliver']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize empty list\n",
        "text_cleaned = []\n",
        "spacy_lst_object = []\n",
        "# iterate over each preprocessed document\n",
        "for idx, row in df_data.iterrows(): \n",
        "    # keep only the lemma for each token which has been tagged as one of these POS tags [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"] AND its lemma IS NOT contained in the stopwords_list AND its lemma has more then 1 character\n",
        "    text = [w.lemma_.lower() for w in row['text_preprocessed'] if w.pos_ in [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"] and w.lemma_.lower() not in stopwords_list and len(w.lemma_.lower())> 1]\n",
        "    text_cleaned.append(text)\n",
        "   \n",
        "df_data[\"text_cleaned\"] = text_cleaned\n",
        "#df_data[\"text_spacy_prepross\"] = spacy_lst_object\n"
      ],
      "metadata": {
        "id": "4DSCDScJee8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Vectorisation of lexical features\n",
        "\n",
        "In the following section, we create the word matrix that will be used in our model transforming a collection of text documents into a matrix of token counts. The following code is used to set the parameters of the matrix that we will create."
      ],
      "metadata": {
        "id": "-A2m2Np-heiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = 5, # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = 450, # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords_list, # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps"
      ],
      "metadata": {
        "id": "rh0pO9WIgApa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The following chunk of code we use the `nltk` learn a vocabulary of all tokens in the raw documents."
      ],
      "metadata": {
        "id": "YWC7liMKMr62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM = vectorized.fit_transform(df_data[\"text_cleaned\"])"
      ],
      "metadata": {
        "id": "fZru5J1zglsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_term_DTM"
      ],
      "metadata": {
        "id": "UzS-0TEEjveo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk of code, we create another matrix using 'scikitilearn' but this time we use the TFIDF as weighting scheme of each word. The TFIDF is calculated as follow:\n",
        "\n",
        "$\n",
        "\\begin{align}\n",
        "&\\text{Let}\\ t = \\text{Term}\\\\\n",
        "&\\text{Let}\\ IDF = \\text{Inverse Document Frequency}\\\\\n",
        "&    \\text{Let}\\ TF =\\text{Term Frequency}\\\\[2em]\n",
        "&    TF \\: =\\: \\frac{\\text{term frequency in document}}{\\text{total words in document}}\\\\[1em]\n",
        "&    IDF(t) \\: =\\: \\log_2\\left(\\frac{\\text{total documents in corpus}}{\\text{documents with term}}\\right)\n",
        "\\end{align}\n",
        "$\n",
        "\n",
        "Then tf–idf is calculated as :\n",
        "$$tfidf( t, d, D ) = tf( t, d ) \\times idf( t, D )$$"
      ],
      "metadata": {
        "id": "bTzg1vPTfKiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This execute the tfidf weigthing\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM = tfidf.fit_transform(freq_term_DTM)"
      ],
      "metadata": {
        "id": "1W4puzyXkA7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a id=\"desc-stat\"></a>\n",
        "# <font size = '6' color='f28c00'>Section 2: Descriptive statistics</font>\n",
        "In this section, we explore the cleaned text of the web pages through descriptive analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "_jutXQxXmMtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Metadata\n",
        "In this section, we extract some statistics about the Metadata"
      ],
      "metadata": {
        "id": "JYFvlJB9prTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We count the number of companies that we have grouping by the sector\n",
        "df_data['sector'].value_counts()"
      ],
      "metadata": {
        "id": "QVmYHQylSkW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We count the number of companies that we have grouping by the industry\n",
        "df_data['industry'].value_counts()"
      ],
      "metadata": {
        "id": "e_ahE5nrWVHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We count the number of companies that we have grouping by the sector and the industry\n",
        "df_data.groupby(['sector','industry']).count()"
      ],
      "metadata": {
        "id": "RYhx_c21TJ67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We count the number of companies that we have grouping by the state and the city\n",
        "df_data.groupby(['state','city']).count()"
      ],
      "metadata": {
        "id": "9gD5J7hdUXB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We count the number of companies that we have in Canada grouping by the state and the city\n",
        "df_data[df_data.country=='Canada'].groupby(['state','city']).count()"
      ],
      "metadata": {
        "id": "6Go1I-uRU1Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the following chunk of code, we do some descriptive analysis of the metadata \"overall_score\" that is a score assign to each company by the BCorp organization. The highest is the score the more virtous is the company"
      ],
      "metadata": {
        "id": "RKhqXDKTzkhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get the mean of the `overall_scoe` grouping by the sector.\n"
      ],
      "metadata": {
        "id": "2y4Jv6wUqwFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .groupby() execute the groping step ans .mean() is the function to average \n",
        "df_data.groupby(['sector'])['overall_score'].mean()"
      ],
      "metadata": {
        "id": "aldDZhJ_lodG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We calculate the average of the overall score of companies that we have in Canada grouping by the state"
      ],
      "metadata": {
        "id": "GC2nFFBJrJxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_data[df_data['country'] == 'Canada'] this part of code allow ti select rows which have values 'Canada' in column 'country'\n",
        "df_data[df_data['country'] == 'Canada'].groupby(['state'])['overall_score'].mean()"
      ],
      "metadata": {
        "id": "zoeGASGXXAiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We extract the basic statistics of the overall score of the companies/webpages that we have in the sector Service with Significant Environmental Footprint"
      ],
      "metadata": {
        "id": "rXQ78x2EruZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .describe() dive basic statistics os a numerical variable\n",
        "df_data[df_data['sector'] == 'Service with Significant Environmental Footprint']['overall_score'].describe()"
      ],
      "metadata": {
        "id": "TbO4jndsXU_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We extract the basic statistics of the overall score of the companies/webpages that we have in the sector Service with Minor Environmental Footprint"
      ],
      "metadata": {
        "id": "sjj31_K9sCZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# .describe() dive basic statistics os a numerical variable\n",
        "df_data[df_data['sector'] == 'Service with Minor Environmental Footprint']['overall_score'].describe()"
      ],
      "metadata": {
        "id": "slXh9OuRYEJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Compare two values of a variable. i.e. tow different industry of the columns `industry`"
      ],
      "metadata": {
        "id": "kylMSdANsHnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting the industry sector to examinate\n",
        "lst_env_sector=[\"Service with Minor Environmental Footprint\", \"Service with Significant Environmental Footprint\"]\n",
        "######Setting the parameters of the graph to plot\n",
        "for sector in lst_env_sector:\n",
        "  # Create a subset with the two sectors\n",
        "  subset = df_data[df_data.sector == sector]\n",
        "  sns.distplot(subset['overall_score'], hist=False, kde=True, kde_kws={'linewidth': 3}, label=sector)\n",
        "##Plotting the graph of the two industry sector under exam\n",
        "plt.legend(prop={'size': 9}, title='Sector')\n",
        "plt.title('Density Plot')\n",
        "plt.xlabel('Overall_score', size=12)\n",
        "plt.ylabel('Density', size=12)\n",
        "plt.xlim(0, 1500)\n",
        "plt.gcf().set_size_inches(8, 6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0P-o_7lf1QfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Textual data"
      ],
      "metadata": {
        "id": "NkMeBy_FmvCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we extract the word frequency from the webpages cleaned and we illustrate them through the wordcloud\n",
        "\n"
      ],
      "metadata": {
        "id": "f-cbYZiO477h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk of code, we need to execute some intermediate operations to prepare data. Essentially, we compute total frequencies of words using the simple frequency weighting, which is stored in the `freq_term_DTM` matrix. "
      ],
      "metadata": {
        "id": "nci1xraEa9U-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get vocabulary\n",
        "vocab = vectorized.vocabulary_\n",
        "# compute frequency of word using freq_term_DTM\n",
        "data_WC = prepare_data_for_WC(freq_term_DTM, vocab)"
      ],
      "metadata": {
        "id": "FMmI4sX_711B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Then, we generate the plot."
      ],
      "metadata": {
        "id": "6CSnV3ou6Xwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and generate a word cloud image:\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HEJ_7ToIdYwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we show the wordcloud considering a specific industry."
      ],
      "metadata": {
        "id": "kIzdPakpceoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorized.vocabulary_\n",
        "# select some rows according to the index of the rows\n",
        "selected_index = df_data[df_data['industry'] == 'Marketing & Communications Services'].index\n",
        "selected_DTM = freq_term_DTM[selected_index]\n",
        "####\n",
        "data_WC = prepare_data_for_WC(selected_DTM, vocab)\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.figsize=(12, 10)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s_xIFZdv9bsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "And then by sector"
      ],
      "metadata": {
        "id": "kIra6C3cALDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorized.vocabulary_\n",
        "# select some rows according to the index of the rows\n",
        "selected_index = df_data[df_data['sector'] == 'Service'].index #################################\n",
        "selected_DTM = freq_term_DTM[selected_index]\n",
        "####\n",
        "data_WC = prepare_data_for_WC(selected_DTM, vocab)\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uXRH6dacAJTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The following chunks of codes create a descriptive graph of the number of words on web pages according to the different categories examinded.Here we analyse all the `sectort` variable."
      ],
      "metadata": {
        "id": "z6p6K7oj0Am1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorized.vocabulary_\n",
        "# select some rows according to the index of the rows\n",
        "selected_index = df_data[df_data['state'] == 'Alberta'].index\n",
        "selected_DTM = freq_term_DTM[selected_index]\n",
        "####\n",
        "data_WC = prepare_data_for_WC(selected_DTM, vocab)\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HqDIABtuiypL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column 'number_words_webpages' with the number of words for each article generated with the \"apply\" function apply the len to all the column   \n",
        "df_data['number_words_webpages'] = df_data[\"text_cleaned\"].apply(len)\n",
        "# Setting the parameters of the graph that we will plot below\n",
        "for sector in df_data.sector.unique():\n",
        "  # Create a subset with the different sectors\n",
        "  subset = df_data[df_data.sector == sector]\n",
        "  sns.distplot(subset['number_words_webpages'],\n",
        "               hist=False,# Boolean value to plot a (normed) histogram.\n",
        "               kde=True, # Boolean value to plot a gaussian kernel density estimate\n",
        "               kde_kws={'linewidth': 3},\n",
        "               label=sector)\n",
        "# Plot the graph with the distribution of the number of words for each webpages in the different sectors\n",
        "plt.legend(prop={'size': 9}, title='Sector')\n",
        "# plt.title('Density Plot for Each Author')\n",
        "plt.figsize=(12, 10)\n",
        "plt.xlabel('Number of words per webpage', size=12)\n",
        "plt.ylabel('Density', size=12)\n",
        "plt.xlim(0, 1500)\n",
        "#plt.gcf().set_size_inches(8, 6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nJ0UUOFfYuN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Here, we execute the same operation, but subsetting only those companies which are in two specific sector: \"Service with Minor Environmental Footprint\" and \"Service with Significant Environmental Footprint\"\n"
      ],
      "metadata": {
        "id": "LuRKjoV3v1Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting the industry sector to examinate\n",
        "lst_env_sector=[\"Service with Minor Environmental Footprint\", \"Service with Significant Environmental Footprint\"]\n",
        "######Setting the parameters of the graph to plot\n",
        "for sector in lst_env_sector:\n",
        "  # Create a subset with the two sectors\n",
        "  subset = df_data[df_data.sector == sector]\n",
        "  sns.distplot(subset['number_words_webpages'], \n",
        "               hist=False, \n",
        "               kde=True, \n",
        "               kde_kws={'linewidth': 3}, \n",
        "               label=sector)\n",
        "##Plotting the graph of the two industry sector under exam\n",
        "plt.legend(prop={'size': 9}, title='Sector')\n",
        "# plt.title('Density Plot for Each Author')\n",
        "plt.xlabel('Number of words per webpage', size=12)\n",
        "plt.ylabel('Density', size=12)\n",
        "plt.xlim(0, 1500)\n",
        "plt.figsize=(12, 10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AyVKD1s5aJEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we execute  the same operation but considering the country and a specific sector"
      ],
      "metadata": {
        "id": "KO9_7D11w1Zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecting the sector to represent in the plot\n",
        "data_MEF=df_data[df_data['sector']==\"Service with Minor Environmental Footprint\"]\n",
        "\n",
        "for country in data_MEF.country.unique():\n",
        "  # Create a subset with the different country\n",
        "  subset = data_MEF[data_MEF.country == country]\n",
        "  sns.distplot(subset['number_words_webpages'], \n",
        "               hist=False, \n",
        "               kde=True, \n",
        "               kde_kws={'linewidth': 3}, \n",
        "               label=country)\n",
        "#Plot the graph highlighting the differences between the countries of the selected sector\n",
        "plt.legend(prop={'size': 14}, title='Country')\n",
        "# plt.title('Density Plot for Each Author')\n",
        "plt.xlabel('Number of words per webpage', size=12)\n",
        "plt.ylabel('Density', size=12)\n",
        "plt.xlim(0, 1500)\n",
        "plt.gcf().set_size_inches(8, 6)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8hadxGPGXu99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "We show a wordcloud looking at only one values in a columns, i.e. one state (Aberta) of columns `state`."
      ],
      "metadata": {
        "id": "Yh9Lb5zRxdSr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coHj_8ZazkCY"
      },
      "source": [
        "<a id=\"analysis\"></a>\n",
        "# Section 3: Analysis \n",
        "\n",
        "In this section, we run the k mean cluster algorithm on the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Document clustering\n"
      ],
      "metadata": {
        "id": "dmlBLX-HQlCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As meantioned in section 1, the goal of the k-means cluster is to group rows in a homogeneous set of clusters minimizing the pairwise squared deviations of the data points in the same cluster. Below, an example of a k-mean clusters with k = 5\n",
        "\n",
        "![k_mean_cluster.png](data:image/png;base64,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)\n",
        "\n",
        "(https://www.ml-science.com/k-means-clustering)\n"
      ],
      "metadata": {
        "id": "LfJmHL6KdA55"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the chuck below, we execute the clustering algorithm on the TF-IDF matrix (`tfidf_DTM`), chosing to generate `20` clusters."
      ],
      "metadata": {
        "id": "WD3qeGa8yuZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cls_kmeans = Clustering_kmeans(DTM = tfidf_DTM, # Matrice to pass under sklearn classifiers\n",
        "                               reduce_lsa = True,# a SVD is executed on the DTM before to execute the clustering\n",
        "                               n_comp_lsa= 300,# Number of component to keep after SVD on DTM.\n",
        "                               n_clusters = 20,# Number of cluster represented in the plot\n",
        "                               random_state = 8426, # Determines random number generation for centroid initialization\n",
        "                               max_iter = 1000, #Maximum number of iterations of the k-means algorithm for a single run\n",
        "                               n_init = 100,# Number of time the k-means algorithm will be run with different centroid seeds\n",
        "                               tol = 0.0001,# Relative tolerance with regards to Frobenius norm of the difference in the cluster centers of two consecutive iterations to declare convergence\n",
        "                               init = \"k-means++\")# selects initial cluster centers for k-mean clustering in a smart way to speed up convergence"
      ],
      "metadata": {
        "id": "5f-NGOycjFVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of documents for each cluster."
      ],
      "metadata": {
        "id": "Q1_zbLVizeh0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the Number of Occurrences in a Python list using Counter\n",
        "from collections import Counter\n",
        "Counter(cls_kmeans.labels_)"
      ],
      "metadata": {
        "id": "eZETT6-XplUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Plot data grouped by clusters"
      ],
      "metadata": {
        "id": "xrZcx8ugkvlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this subsection, we plot the result of the cluster in a 2D representation"
      ],
      "metadata": {
        "id": "IU_3NTXadm0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data_by_cluster(DTM = tfidf_DTM, cls_kmean = cls_kmeans)"
      ],
      "metadata": {
        "id": "jjyNGiU8fJJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Wordcloud by cluster\n",
        "\n",
        "We use Tf-IDF weights to plot most important words for each cluster."
      ],
      "metadata": {
        "id": "2TSBo--psyda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud_par_cluster(wordcloud = WordCloud(), # A WordCloud function. \n",
        "                      DTM = tfidf_DTM,# A Document-Term Matrix \n",
        "                      cls_kmeans, ###############################\n",
        "                      vocab = vectorized.vocabulary_, # a vocabulary from scikitlearn vectorizer\n",
        "                      first_n_words=10,#  It indicates how many words to print\n",
        "                      figsize=(12, 10),\n",
        "                      fontsize=32,\n",
        "                      plot_wordcloud = False,\n",
        "                      lst_clust = [],\n",
        "                      title_in_plot = \"Clust_\")"
      ],
      "metadata": {
        "id": "40mECfJP4TK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Statistics for Lexical Keyness\n"
      ],
      "metadata": {
        "id": "6Thb9kfIO6mH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A complemtary analysis of the sum of Tf-IDF weigths is <font color='f28c00'>keyness</font>.  This analysis gives the statistical significance of a keyword's frequency in a given corpus, relative to a reference corpus.\n",
        "\n",
        "In our case, we compare all the words of the documents of a cluster against the other docuemnts of the corpus. The Log-likelihood Ratio return the lexical specificity of the word for the target cluster. \n",
        "\n",
        "Keyness is generally sensible to low frequency. For this reason we can also sort Log-likelihood Ratio after having removed low frequent words.\n",
        "\n",
        "A short description of the method is accesible [here](https://alvinntnu.github.io/NTNU_ENC2036_LECTURES/keyword-analysis.html#statistics-for-keyness)."
      ],
      "metadata": {
        "id": "QdOjyAwQN1bW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Keyness is generally sensible to low frequency. "
      ],
      "metadata": {
        "id": "R5Rt_pyWekyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_keyness = lexical_keyness(freq_term_DTM, cls_kmeans, 13)+\n",
        "df_keyness[df_keyness['tot']>50]['Log-likelihood Ratio'].sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "FeoqohmFdSIW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Cross-Analysis of the cluster results with the metadata\n",
        "\n",
        "We can use metadata to deeper explore clusters created by the Kmeans. \n",
        "\n",
        "In this subsection, we analyze the `overall_score` and the `sector` of the companies for each cluster. "
      ],
      "metadata": {
        "id": "PQgWZQPvzdBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "in the chunk below, we preapare data for analysis."
      ],
      "metadata": {
        "id": "2ol9xFY9d4VF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a new column to assign to each company the correspondant cluster\n",
        "df_data['cluster_labels'] = cls_kmeans.labels_"
      ],
      "metadata": {
        "id": "kTKKy_0x2ExY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The following chunk of code calculate the mean of the overall score of a specific cluster of document"
      ],
      "metadata": {
        "id": "V8o9pdVL08X-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.loc[cls_kmeans.labels_== 10, 'overall_score'].mean()"
      ],
      "metadata": {
        "id": "zDTXpUcfzcGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The chunk of code below creates a box plot based on the overall score for each cluster"
      ],
      "metadata": {
        "id": "LPsKYbsX1bIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.boxplot('overall_score', by='cluster_labels', figsize=(12, 8))"
      ],
      "metadata": {
        "id": "nzR1HpCL11ZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the next chunk of code, for comparing reasons, we create a box plot for each sector based on the overall score"
      ],
      "metadata": {
        "id": "vjiTcy-t2_bm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.boxplot('overall_score', by='sector', figsize=(24, 8))"
      ],
      "metadata": {
        "id": "BcjCQe-k4TsC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Then, we calculate the mean of the overall score for each cluster"
      ],
      "metadata": {
        "id": "hq8fr0_d3UrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_score_cluster = df_data[['cluster_labels','overall_score']].groupby('cluster_labels').mean()#['overall_score'].sort_values()"
      ],
      "metadata": {
        "id": "N4ODDj5rmqZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import median \n",
        "######Setting the parameters of the graph to plot\n",
        "sns.barplot(data = mean_score_cluster, x=mean_score_cluster.index, y=mean_score_cluster[\"overall_score\"], estimator=median)\n",
        "##Plotting the graph of the two industry sector under exam\n",
        "plt.legend(prop={'size': 9}, title='Cluster')\n",
        "# plt.title('Density Plot for Each Author')\n",
        "plt.xlabel('Clusters', size=12)\n",
        "plt.ylabel('Overall_score', size=12)\n",
        "plt.figure(figsize=(12, 10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5au6NaXtzHUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Here, we extract the median of the overall score for each cluster"
      ],
      "metadata": {
        "id": "bkebNwsa2Jwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data[['cluster_labels','overall_score']].groupby('cluster_labels').median()#['overall_score'].sort_values()"
      ],
      "metadata": {
        "id": "m44iuBgk3zTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we generate a cropss table of `sector` and `cluster_labels`. We use the compute ration insertad of simple frequency. Thus, we get the propotion of sector for each cluster."
      ],
      "metadata": {
        "id": "bxAlllRBzDDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nominal_metadata_to_analyse = 'sector'\n",
        "df_test = pd.DataFrame(columns=set(df_data[nominal_metadata_to_analyse])) \n",
        "for c_t_ in range(cls_kmeans.n_clusters):\n",
        "    df_test.loc[c_t_] = df_data.loc[cls_kmeans.labels_== c_t_, nominal_metadata_to_analyse].value_counts()\n",
        "df_test = df_test.fillna(0)\n",
        "#\n",
        "# Ratio\n",
        "for col in df_test.columns:\n",
        "    df_test[col] = df_test[col] / df_test[col].sum() * 100\n",
        "\n",
        "df_test = df_test.round(2)\n",
        "df_test"
      ],
      "metadata": {
        "id": "NsnkQBQovLXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Retrive most representatives documents by cluster\n",
        "\n",
        "In this subsection, we retrieve the most representative documents for each cluster.\n",
        "\n",
        "In the chunk below we prepare data and any output will be printed."
      ],
      "metadata": {
        "id": "NdtRkHbxSZ7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "df_data['Cosine_of_related_cluster'] = 0\n",
        "for idx, centroid in enumerate(cls_kmeans.cluster_centers_):\n",
        "    idx_cluster = cls_kmeans.labels_ == idx\n",
        "    similarity = cosine_similarity(cls_kmeans.DTM_[idx_cluster], centroid.reshape(1, -1)).flatten()\n",
        "    df_data.loc[idx_cluster, 'Cosine_of_related_cluster'] = similarity "
      ],
      "metadata": {
        "id": "_1RTfSmHSnqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Below, we print docuements for a specific cluster "
      ],
      "metadata": {
        "id": "BPFFNgwh3bdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## chose cluster number to analyse\n",
        "n_clust = 5\n",
        "df_data[df_data[\"cluster_labels\"] == n_clust].sort_values(by='Cosine_of_related_cluster', ascending= False)[['Cosine_of_related_cluster',\"text_web_page\"]]"
      ],
      "metadata": {
        "id": "CEp1b5WnZu2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LE CODE EN DESSOUS EST ENCORE SOUS ÉVALUATION"
      ],
      "metadata": {
        "id": "zri0HpcB338m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"solar\", \"energy\", \"installation\"]\n",
        "testo=df_data[df_data[\"cluster_labels\"] == 5].sort_values(by='Cosine_of_related_cluster', ascending= False)[\"text_web_page\"].iloc[0]\n",
        "print(testo)"
      ],
      "metadata": {
        "id": "HEFECQZ5v_Iu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#re.findall(\"({0, 200}.)(\"+\")|(\".join(words) + \")(.{0, 200})\", testo)\n",
        "re.findall(\"(\".join(words) + \").+\", testo)[0]"
      ],
      "metadata": {
        "id": "DaQpZhThxbKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"(\"+\")|(\".join(words) + \")\""
      ],
      "metadata": {
        "id": "EANNLsNByytI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.iloc[closeness_scores]['Cosine_of_related_cluster']"
      ],
      "metadata": {
        "id": "JDnneOilZ_L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in close_idx_similarity_tpl:\n",
        "    print(list(x))\n",
        "    break"
      ],
      "metadata": {
        "id": "u-ErW_u2V5jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_data.iloc[136]['text_web_page']\n",
        "# df_data.iloc[136]['description']\n",
        "# df_data.columns"
      ],
      "metadata": {
        "id": "4FQAjn-pX8hZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Exercice\n",
        "\n",
        "In this section paticipants challenge themself reproducing the steps made in section 1, 2 and 3. The exercise is accomplished by groups. Each group **will present the final result of their work** in no more then 5 minutes at the end of the workshop. Below, we describe the steps of the exercice.\n",
        "\n",
        "In the first phase (10 minutes maximum), each group has to chose a sub-corpus of the whole corpus using metadata, such as `sector`, `industry`, `country`, `state` or `city`. They can chose a combination of them, for exemple, a sub-corpus containing companies of the agriculture sector of Canada. For analysis of textual data, groups can choose between two different textual data field : \n",
        "1. `text_web_page`, which contains data collected by WaybackMachine and used for the first part of this workshop \n",
        "2. `description`, which contain a short description of companies, provided by them to BCorporation organization.\n",
        "\n",
        "In the second phase (40 minutes), groups will pass trougth all the analytical steps, from section 4.2 to section 4.8. In those sections, we provide you all the necessary chunks of code in order to succeed the exercise. In the most of these chunk of code, you only need to fill some empty part, which are indicated with the three points `...`. Occasionally, you could modify other part of the code (for intermediate level participants only).\n",
        "\n",
        "In the third phase (10 minutes), groups has to complete the interpretation of the results and prepare one or two points they chose to show to the class. \n",
        "\n",
        "Belolw, we show the two textual field you can choose for the exercice."
      ],
      "metadata": {
        "id": "eXZZgCj2OWB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data[\"description\", \"text_web_page\"]"
      ],
      "metadata": {
        "id": "8HL46HTdOWB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Select your sub-corpus\n",
        "\n",
        "Look at metadata and, after discussion with your group, select the sub-corpus you want to explore. Be sure to have enough companies in your sub-corpus (around 100 at least). \n",
        "\n",
        "Don't be afraid to ask help to your monitor for this part!!\n",
        "\n",
        "The folowing chunk give you an exemple of subsection. PAy attention the folowng subsection do not contain encoug document.\n",
        "\n",
        "Below, we provide an example of subsection of rows, regarding to sector. We assign the result of the subsection to a new dataset named `df_data_selection`."
      ],
      "metadata": {
        "id": "qIiO9ZT2OeXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_selection = df_data[df_data[\"sector\"]=='service']"
      ],
      "metadata": {
        "id": "0iLd4oahOdgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Morphological Analysis\n",
        "For the morphological analysis you can choose between two textual data field: `text_web_page`,`description`. Fill the empty part `...` with the chosen textual column of the subset created above"
      ],
      "metadata": {
        "id": "6Qw6TV8cD6Ni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_data_selection['text_preprocessed'] = list(nlp.pipe(df_data_selection[...], disable = [\"tok2vec\",'parser','ner']))"
      ],
      "metadata": {
        "id": "3XmDQxHaD6Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Filter lexical feature\n",
        "You can add words of your choice to the stopword list. This is not necessary to succeed the exercice, but can help if you want to remove non significant word which appear later in your analysis."
      ],
      "metadata": {
        "id": "7p7L7uHDSqdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download default list of stopword\n",
        "nltk.download('stopwords')\n",
        "# add custum words to the stopword list\n",
        "stopwords_list = set(stopwords.words('english') + ['would'])"
      ],
      "metadata": {
        "id": "z6DltLfFSp7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Fill the empty part `...` with the list of POS tag you want to include in your feature selection"
      ],
      "metadata": {
        "id": "bci0ibz3TN52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize empty list\n",
        "text_cleaned = []\n",
        "spacy_lst_object = []\n",
        "# iterate over each preprocessed document\n",
        "for idx, row in df_data_selection.iterrows(): \n",
        "    # keep only the lemma for each token which has been tagged as one of these POS tags [\"ADV\", \"ADJ\", \"NOUN\", \"VERB\"] AND its lemma IS NOT contained in the stopwords_list AND its lemma has more then 1 character\n",
        "    text = [w.lemma_.lower() for w in row['text_preprocessed'] if w.pos_ in [...] and w.lemma_.lower() not in stopwords_list and len(w.lemma_.lower())> 1]\n",
        "    text_cleaned.append(text)\n",
        "   \n",
        "df_data_selection[\"text_cleaned\"] = text_cleaned\n",
        "#df_data[\"text_spacy_prepross\"] = spacy_lst_object\n"
      ],
      "metadata": {
        "id": "CicbNrbHTM9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Vectorisatiom of lexical features\n",
        "For this step, you have to choose the values for `min_df` and `max_df` arguments. Fill the empty part `...`. At the end of the next chunk of code we generate the frequency matrix and the Tf-IDF matrix. They are named as follow: \n",
        "1. `freq_term_DTM_selection`\n",
        "2. `tfidf_DTM_selection`\n",
        "\n",
        "The dimensions of the `freq_term_DTM_selection` is printed in order to give you an idea of the number of terms you are keeping in your matrix."
      ],
      "metadata": {
        "id": "BYP-Iiz1T2KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def identity_tokenizer(text):\n",
        "    return text\n",
        "\n",
        "# Transforming the word in frequencies\n",
        "vectorized = CountVectorizer(lowercase = False, # Convert all characters to lowercase before tokenizing\n",
        "                             min_df = ..., # Ignore terms that have a document frequency strictly lower than the given threshold \n",
        "                             max_df = ..., # Ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words)\n",
        "                             stop_words = stopwords_list, # Remove the list of words provided\n",
        "                             ngram_range = (1, 1), # Get the lower and upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted\n",
        "                             tokenizer=identity_tokenizer) # Override the string tokenization step while preserving the preprocessing and n-grams generation steps\n",
        "#\n",
        "freq_term_DTM_selection = vectorized.fit_transform(df_data_selection[\"text_cleaned\"])\n",
        "tfidf = TfidfTransformer(norm='l1')\n",
        "tfidf_DTM_selection = tfidf.fit_transform(freq_term_DTM_selection)\n",
        "#\n",
        "freq_term_DTM_selection"
      ],
      "metadata": {
        "id": "gePRWwGZT1kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Descriptive statistics"
      ],
      "metadata": {
        "id": "0-XJ5piIV3l4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the wordcloud of most frequent words your sub-corpus. Fill the empty part `...` with the frequency matrix you generated in the previous chunk."
      ],
      "metadata": {
        "id": "2lROewv0_sXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorized.vocabulary_\n",
        "data_WC = prepare_data_for_WC(..., vocab) # Fill with the frequency matrix\n",
        "# Create and generate a word cloud image:\n",
        "wordcloud = WordCloud().generate_from_frequencies(data_WC)\n",
        "# Display the generated image:\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ckfyj7D5Vufi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.6 Document clustering\n",
        "\n",
        "Here, we execute the clustering algorithm. Fill the empty part `...`. We generate the variable `kmeans_result_selection` containing the result of the clustering."
      ],
      "metadata": {
        "id": "K5bOvCxhQuVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans_result_selection = Clustering_kmeans(DTM =..., # Fill with the tfidf matrix\n",
        "                               reduce_lsa = True,\n",
        "                               n_comp_lsa= 300,\n",
        "                               n_clusters = ..., # choose the number of clusters to generate\n",
        "                               random_state = 8426,\n",
        "                               max_iter = 1000,\n",
        "                               n_init = 100,\n",
        "                               tol = 0.0001,\n",
        "                               init = \"k-means++\")"
      ],
      "metadata": {
        "id": "oGEWQP7AQt9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.7 Wordcloud by cluster\n",
        "\n",
        "We analyse the most important words for each cluster generate above. We use tthe Tf-IDF matrix for this operation."
      ],
      "metadata": {
        "id": "NPiNA_rlQ1fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud_par_cluster(WordCloud(), # \n",
        "                      ..., # Fill with the tfidf matrix of your sub-corpus\n",
        "                      ..., # Fill with the results of the clustering\n",
        "                      vectorized.vocabulary_, # \n",
        "                      first_n_words = ...,\n",
        "                      figsize = (12, 10),\n",
        "                      fontsize = 32,\n",
        "                      plot_wordcloud = False, #\n",
        "                      lst_clust = [], #\n",
        "                      title_in_plot = \"Clust_\") #"
      ],
      "metadata": {
        "id": "zulRsZp_Q1fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.8 Read most relevant document by cluster\n"
      ],
      "metadata": {
        "id": "acDWHwdARwq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CODE TO BE COMPLETED"
      ],
      "metadata": {
        "id": "rRja-PvxR90W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.9 Complete the exercise\n",
        "\n",
        "Prepare your presentation for plenary session. You have 5 minutes to present one or two element of your group work. "
      ],
      "metadata": {
        "id": "hfOf7_iLR-MW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Workshop_4POINT0_Test.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}